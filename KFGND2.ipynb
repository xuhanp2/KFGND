{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Device: cpu\n"
     ]
    }
   ],
   "source": [
    "# Cell 1\n",
    "import math\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.fft\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader, TensorDataset\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "plt.rcParams[\"figure.figsize\"] = (8,5)\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(\"Device:\", device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 2: Burgers PDE solver (pseudo-spectral, periodic BC) + GRF initial condition generator\n",
    "# We'll solve u_t + u u_x = nu u_xx on x in [0,1], with periodic BC, integrate to t=1 using RK4.\n",
    "\n",
    "# Generate the initial value\n",
    "def sample_grf(n_points, lengthscale=0.05, scale=1.0, seed=None):\n",
    "    if seed is not None:\n",
    "        np.random.seed(seed)\n",
    "    k = np.fft.fftfreq(n_points, d=1.0/n_points) \n",
    "    power = np.exp(-(2*np.pi*k*lengthscale)**2)\n",
    "    phases = np.random.normal(size=(n_points,)) + 1j * np.random.normal(size=(n_points,))\n",
    "    ft = phases * np.sqrt(power)\n",
    "    x = np.fft.ifft(ft).real\n",
    "    x = x - x.mean()\n",
    "    x = (x / x.std()) * scale\n",
    "    return x\n",
    "\n",
    "# Integrate Burgers with pseudospectral and RK4. Return u(x,1)\n",
    "def burgers_pseudospectral(u0, nu, n_steps=200, dt=None):\n",
    "    N = u0.size\n",
    "    x = np.linspace(0,1,N,endpoint=False)\n",
    "    k = 2*np.pi*np.fft.fftfreq(N, d=1.0/N)\n",
    "    k2 = k**2\n",
    "    if dt is None:\n",
    "        dt = 1.0/n_steps\n",
    "    u = u0.copy()\n",
    "    def rhs(u):\n",
    "        uhat = np.fft.fft(u)\n",
    "        ux = np.fft.ifft(1j * k * uhat).real\n",
    "        nonlinear = - u * ux\n",
    "        visc = nu * np.fft.ifft(-k2 * uhat).real\n",
    "        return nonlinear + visc\n",
    "\n",
    "    t = 0.0\n",
    "    for _ in range(n_steps):\n",
    "        k1 = rhs(u)\n",
    "        k2v = rhs(u + 0.5*dt*k1)\n",
    "        k3 = rhs(u + 0.5*dt*k2v)\n",
    "        k4 = rhs(u + dt*k3)\n",
    "        u = u + (dt/6.0)*(k1 + 2*k2v + 2*k3 + k4)\n",
    "        t += dt\n",
    "    return u\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating dataset (this can take a while)... total samples: 1200\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/1200 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pjw10\\OneDrive\\ipykernel_16808\\1826202861.py:29: RuntimeWarning: overflow encountered in multiply\n",
      "  nonlinear = - u * ux\n",
      "C:\\Users\\pjw10\\AppData\\Local\\Packages\\PythonSoftwareFoundation.Python.3.11_qbz5n2kfra8p0\\LocalCache\\local-packages\\Python311\\site-packages\\numpy\\fft\\_pocketfft.py:94: RuntimeWarning: invalid value encountered in fft\n",
      "  return ufunc(a, fct, axes=[(axis,), (), (axis,)], out=out)\n",
      "100%|██████████| 1200/1200 [03:18<00:00,  6.04it/s]\n"
     ]
    }
   ],
   "source": [
    "# Cell 3 generate training dataset\n",
    "\n",
    "def make_dataset(n_train=1000, n_test=200, n_points=1024, nu=1e-2, grf_lenscale=0.03):\n",
    "    X_train = []\n",
    "    Y_train = []\n",
    "    X_test = []\n",
    "    Y_test = []\n",
    "    total = n_train + n_test\n",
    "    print(\"Generating dataset (this can take a while)... total samples:\", total)\n",
    "    for i in tqdm(range(total)):\n",
    "        u0 = sample_grf(n_points, lengthscale=grf_lenscale, scale=1.0)\n",
    "        u1 = burgers_pseudospectral(u0, nu, n_steps=400)  \n",
    "        if i < n_train:\n",
    "            X_train.append(u0.astype(np.float32))\n",
    "            Y_train.append(u1.astype(np.float32))\n",
    "        else:\n",
    "            X_test.append(u0.astype(np.float32))\n",
    "            Y_test.append(u1.astype(np.float32))\n",
    "    X_train = np.stack(X_train)\n",
    "    Y_train = np.stack(Y_train)\n",
    "    X_test = np.stack(X_test)\n",
    "    Y_test = np.stack(Y_test)\n",
    "    return X_train, Y_train, X_test, Y_test\n",
    "\n",
    "X_train, Y_train, X_test, Y_test = make_dataset(\n",
    "    n_train=1000, n_test=200, n_points=1024\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 4 FNO in 1D\n",
    "\n",
    "class SpectralConv1d(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, modes):\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.modes = modes\n",
    "        self.scale = 1 / (in_channels * out_channels)\n",
    "        self.weight = nn.Parameter(self.scale * torch.randn(in_channels, out_channels, self.modes, 2))\n",
    "\n",
    "    def compl_mul1d(self, input, weight):\n",
    "        a, b = input[...,0], input[...,1]\n",
    "        c, d = weight[...,0], weight[...,1]\n",
    "        real = torch.einsum(\"bim, iom -> bom\", a, c) - torch.einsum(\"bim, iom -> bom\", b, d)\n",
    "        imag = torch.einsum(\"bim, iom -> bom\", a, d) + torch.einsum(\"bim, iom -> bom\", b, c)\n",
    "        return torch.stack((real, imag), dim=-1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch, channels, n)\n",
    "        batchsize, channels, n = x.shape\n",
    "        x_ft = torch.fft.rfft(x, dim=-1) \n",
    "        out_ft = torch.zeros(batchsize, self.out_channels, x_ft.shape[-1], 2, device=x.device)\n",
    "        modes = min(self.modes, x_ft.shape[-1])\n",
    "        input_modes = torch.stack((x_ft[..., :modes].real, x_ft[..., :modes].imag), dim=-1) \n",
    "        w = self.weight \n",
    "        out_modes = self.compl_mul1d(input_modes, w.permute(0,1,2,3))\n",
    "        out_ft[..., :modes, :] = out_modes\n",
    "        real = out_ft[...,0]  # (b, out, nfreqs)\n",
    "        imag = out_ft[...,1]\n",
    "        complex_ft = torch.complex(real, imag)\n",
    "        x_out = torch.fft.irfft(complex_ft, n=n, dim=-1)\n",
    "        return x_out\n",
    "\n",
    "class FNO1d(nn.Module):\n",
    "    def __init__(self, modes=16, width=64, depth=4):\n",
    "        super().__init__()\n",
    "        self.modes = modes\n",
    "        self.width = width\n",
    "        self.depth = depth\n",
    "\n",
    "        self.input_proj = nn.Linear(1, self.width)\n",
    "        self.spectral_layers = nn.ModuleList()\n",
    "        self.w_conv = nn.ModuleList()\n",
    "        for _ in range(self.depth):\n",
    "            self.spectral_layers.append(SpectralConv1d(self.width, self.width, modes))\n",
    "            self.w_conv.append(nn.Conv1d(self.width, self.width, kernel_size=1))\n",
    "\n",
    "        self.output_proj = nn.Sequential(\n",
    "            nn.Linear(self.width, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(128, 1)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        # x: (batch, n_points)\n",
    "        b, n = x.shape\n",
    "        x = x.unsqueeze(-1)  # (b, n, 1)\n",
    "        x = self.input_proj(x)  # (b, n, width)\n",
    "        x = x.permute(0,2,1)  # (b, width, n)\n",
    "        for spec, w in zip(self.spectral_layers, self.w_conv):\n",
    "            x1 = spec(x)  # (b, width, n)\n",
    "            x2 = w(x)  # (b, width, n)\n",
    "            x = x1 + x2\n",
    "            x = torch.relu(x)\n",
    "        x = x.permute(0,2,1)  # (b, n, width)\n",
    "        x = self.output_proj(x)  # (b, n, 1)\n",
    "        return x.squeeze(-1)  # (b, n)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 5\n",
    "\n",
    "class KFNGD:\n",
    "    def __init__(self, model, lr=1e-3, damping=1e-2, ema_decay=0.05):\n",
    "        self.model = model\n",
    "        self.lr = lr\n",
    "        self.damping = damping\n",
    "        self.ema_decay = ema_decay\n",
    "        self.layer_infos = []\n",
    "        self._register_layers()\n",
    "\n",
    "    def _register_layers(self):\n",
    "        for name, module in self.model.named_modules():\n",
    "            if isinstance(module, nn.Linear):\n",
    "                info = {\n",
    "                    'name': name, 'module': module,\n",
    "                    'A': None, 'G': None\n",
    "                }\n",
    "                self.layer_infos.append(info)\n",
    "            elif isinstance(module, nn.Conv1d) and module.kernel_size == (1,):\n",
    "                info = {\n",
    "                    'name': name, 'module': module,\n",
    "                    'A': None, 'G': None\n",
    "                }\n",
    "                self.layer_infos.append(info)\n",
    "        for info in self.layer_infos:\n",
    "            mod = info['module']\n",
    "            def forward_hook(module, input, output, info=info):\n",
    "                info['a'] = input[0].detach().clone()\n",
    "            mod.register_forward_hook(forward_hook)\n",
    "            def backward_hook(module, grad_input, grad_output, info=info):\n",
    "                info['g'] = grad_output[0].detach().clone()\n",
    "            mod.register_full_backward_hook(backward_hook)\n",
    "\n",
    "    def step(self, batch_size):\n",
    "        for info in self.layer_infos:\n",
    "            mod = info['module']\n",
    "            a = info.get('a', None)\n",
    "            g = info.get('g', None)\n",
    "            if a is None or g is None:\n",
    "                continue\n",
    "            if isinstance(mod, nn.Linear):\n",
    "                a_flat = a.contiguous().reshape(a.shape[0], -1)\n",
    "                g_flat = g.contiguous().reshape(g.shape[0], -1)\n",
    "            else:\n",
    "                a_flat = a.permute(0,2,1).contiguous().reshape(-1, a.shape[1])\n",
    "                g_flat = g.permute(0,2,1).contiguous().reshape(-1, g.shape[1])\n",
    "            A_batch = (a_flat.t() @ a_flat) / (a_flat.shape[0])\n",
    "            G_batch = (g_flat.t() @ g_flat) / (g_flat.shape[0])\n",
    "            if info['A'] is None:\n",
    "                info['A'] = A_batch\n",
    "                info['G'] = G_batch\n",
    "            else:\n",
    "                info['A'] = (1.0 - self.ema_decay) * info['A'] + self.ema_decay * A_batch\n",
    "                info['G'] = (1.0 - self.ema_decay) * info['G'] + self.ema_decay * G_batch\n",
    "\n",
    "            damping = self.damping\n",
    "            A_inv = torch.linalg.inv(info['A'] + damping * torch.eye(info['A'].shape[0], device=info['A'].device))\n",
    "            G_inv = torch.linalg.inv(info['G'] + damping * torch.eye(info['G'].shape[0], device=info['G'].device))\n",
    "\n",
    "            if w.grad is None:\n",
    "                continue\n",
    "            grad_W = w.grad.detach()\n",
    "            if isinstance(mod, nn.Conv1d) and mod.kernel_size == (1,):\n",
    "                grad_W_mat = grad_W.squeeze(-1)\n",
    "            else:\n",
    "                grad_W_mat = grad_W\n",
    "            precond = G_inv @ grad_W_mat @ A_inv\n",
    "            if isinstance(mod, nn.Conv1d) and mod.kernel_size == (1,):\n",
    "                mod.weight.grad.copy_(precond.unsqueeze(-1))\n",
    "            else:\n",
    "                mod.weight.grad.copy_(precond)\n",
    "\n",
    "            if mod.bias is not None and mod.bias.grad is not None:\n",
    "                grad_b = mod.bias.grad.detach()\n",
    "                precond_b = G_inv @ grad_b.unsqueeze(-1)\n",
    "                mod.bias.grad.copy_(precond_b.squeeze(-1))\n",
    "\n",
    "            info['a'] = None\n",
    "            info['g'] = None\n",
    "\n",
    "        # apply gradient descent step\n",
    "        with torch.no_grad():\n",
    "            for p in self.model.parameters():\n",
    "                if p.grad is None:\n",
    "                    continue\n",
    "                p -= self.lr * p.grad\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 6 training FNO, adam, sgd, kf-ngd.\n",
    "from copy import deepcopy\n",
    "\n",
    "def relative_l2(u_true, u_pred):\n",
    "    num = np.linalg.norm((u_true - u_pred).reshape(u_true.shape[0], -1), axis=1)\n",
    "    den = np.linalg.norm(u_true.reshape(u_true.shape[0], -1), axis=1)\n",
    "    rel = num / den\n",
    "    return np.mean(rel)\n",
    "\n",
    "def train_and_evaluate(model, optimizer_name, train_loader, test_loader, num_epochs=50, lr=1e-3, damping=1e-2, gamma=0.05):\n",
    "    model = deepcopy(model).to(device)\n",
    "    criterion = nn.MSELoss()\n",
    "    logs = {'train_loss': []}\n",
    "    if optimizer_name.lower() == 'adam':\n",
    "        opt = optim.Adam(model.parameters(), lr=lr)\n",
    "        kf = None\n",
    "    elif optimizer_name.lower() == 'sgd':\n",
    "        opt = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
    "        kf = None\n",
    "    elif optimizer_name.lower() == 'kf-ngd':\n",
    "        opt = None\n",
    "        kf = KFNGD(model, lr=lr, damping=damping, ema_decay=gamma)\n",
    "    else:\n",
    "        raise ValueError(\"Unknown optimizer\")\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        running_loss = 0.0\n",
    "        count = 0\n",
    "        for xb, yb in train_loader:\n",
    "            xb = xb.to(device)\n",
    "            yb = yb.to(device)\n",
    "            opt_used = opt\n",
    "            if opt_used is not None:\n",
    "                opt_used.zero_grad()\n",
    "            else:\n",
    "                # if using KF-NGD, still zero grads\n",
    "                for p in model.parameters():\n",
    "                    if p.grad is not None:\n",
    "                        p.grad.detach_()\n",
    "                        p.grad.zero_()\n",
    "            preds = model(xb)\n",
    "            loss = criterion(preds, yb)\n",
    "            loss.backward()\n",
    "            if optimizer_name.lower() == 'kf-ngd':\n",
    "                # use kf step: it will precondition grads and update params\n",
    "                kf.step(batch_size=xb.shape[0])\n",
    "            else:\n",
    "                opt_used.step()\n",
    "            running_loss += loss.item() * xb.shape[0]\n",
    "            count += xb.shape[0]\n",
    "        avg_loss = running_loss / count\n",
    "        logs['train_loss'].append(avg_loss)\n",
    "\n",
    "        # evaluate on test every epoch (could be less frequent)\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "            y_preds = []\n",
    "            y_trues = []\n",
    "            for xb_t, yb_t in test_loader:\n",
    "                xb_t = xb_t.to(device)\n",
    "                yb_t = yb_t.to(device)\n",
    "                out = model(xb_t).cpu().numpy()\n",
    "                y_preds.append(out)\n",
    "                y_trues.append(yb_t.cpu().numpy())\n",
    "            y_preds = np.concatenate(y_preds, axis=0)\n",
    "            y_trues = np.concatenate(y_trues, axis=0)\n",
    "            rel_l2 = relative_l2(y_trues, y_preds)\n",
    "        print(f\"Epoch {epoch+1}/{num_epochs} | Train Loss: {avg_loss:.6e} | Test RelL2: {rel_l2:.6e}\")\n",
    "    return model, logs, rel_l2\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FNO1d(\n",
      "  (input_proj): Linear(in_features=1, out_features=64, bias=True)\n",
      "  (spectral_layers): ModuleList(\n",
      "    (0-3): 4 x SpectralConv1d()\n",
      "  )\n",
      "  (w_conv): ModuleList(\n",
      "    (0-3): 4 x Conv1d(64, 64, kernel_size=(1,), stride=(1,))\n",
      "  )\n",
      "  (output_proj): Sequential(\n",
      "    (0): Linear(in_features=64, out_features=128, bias=True)\n",
      "    (1): ReLU()\n",
      "    (2): Linear(in_features=128, out_features=1, bias=True)\n",
      "  )\n",
      ")\n",
      "\n",
      "\n",
      "=== Training with adam ===\n",
      "Epoch 1/10 | Train Loss: nan | Test RelL2: nan\n",
      "Epoch 2/10 | Train Loss: nan | Test RelL2: nan\n",
      "Epoch 3/10 | Train Loss: nan | Test RelL2: nan\n",
      "Epoch 4/10 | Train Loss: nan | Test RelL2: nan\n",
      "Epoch 5/10 | Train Loss: nan | Test RelL2: nan\n",
      "Epoch 6/10 | Train Loss: nan | Test RelL2: nan\n",
      "Epoch 7/10 | Train Loss: nan | Test RelL2: nan\n",
      "Epoch 8/10 | Train Loss: nan | Test RelL2: nan\n",
      "Epoch 9/10 | Train Loss: nan | Test RelL2: nan\n",
      "Epoch 10/10 | Train Loss: nan | Test RelL2: nan\n",
      "\n",
      "\n",
      "=== Training with sgd ===\n",
      "Epoch 1/10 | Train Loss: nan | Test RelL2: nan\n",
      "Epoch 2/10 | Train Loss: nan | Test RelL2: nan\n",
      "Epoch 3/10 | Train Loss: nan | Test RelL2: nan\n",
      "Epoch 4/10 | Train Loss: nan | Test RelL2: nan\n",
      "Epoch 5/10 | Train Loss: nan | Test RelL2: nan\n",
      "Epoch 6/10 | Train Loss: nan | Test RelL2: nan\n",
      "Epoch 7/10 | Train Loss: nan | Test RelL2: nan\n",
      "Epoch 8/10 | Train Loss: nan | Test RelL2: nan\n",
      "Epoch 9/10 | Train Loss: nan | Test RelL2: nan\n",
      "Epoch 10/10 | Train Loss: nan | Test RelL2: nan\n",
      "\n",
      "\n",
      "=== Training with kf-ngd ===\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\pjw10\\OneDrive\\ipykernel_16808\\1881010583.py:44: UserWarning: Full backward hook is firing when gradients are computed with respect to module outputs since no inputs require gradients. See https://docs.pytorch.org/docs/main/generated/torch.nn.Module.html#torch.nn.Module.register_full_backward_hook for more details.\n",
      "  loss.backward()\n"
     ]
    }
   ],
   "source": [
    "# Cell 7\n",
    "\n",
    "# run quick\n",
    "n_points = 256\n",
    "n_train = 200\n",
    "n_test = 50\n",
    "num_epochs = 10\n",
    "batch_size = 16\n",
    "\n",
    "# full-scale \n",
    "# n_points = 1024\n",
    "# n_train = 1000\n",
    "# n_test = 200\n",
    "# num_epochs = 30\n",
    "# batch_size = 16\n",
    "\n",
    "# 检查数据是否存在\n",
    "try:\n",
    "    X_train, Y_train, X_test, Y_test\n",
    "except NameError:\n",
    "    print(\"please run make_dataset() generate: X_train, Y_train, X_test, Y_test\")\n",
    "else:\n",
    "    # Generate DataLoader\n",
    "    train_ds = TensorDataset(torch.from_numpy(X_train).float(), torch.from_numpy(Y_train).float())\n",
    "    test_ds = TensorDataset(torch.from_numpy(X_test).float(), torch.from_numpy(Y_test).float())\n",
    "    train_loader = DataLoader(train_ds, batch_size=batch_size, shuffle=True)\n",
    "    test_loader = DataLoader(test_ds, batch_size=batch_size, shuffle=False)\n",
    "    # Create the FNO1d\n",
    "    base_model = FNO1d(modes=16, width=64, depth=4)\n",
    "    print(base_model)\n",
    "    # 运行训练实验（Adam / SGD / KF-NGD）\n",
    "    results = {}\n",
    "    for opt_name in ['adam', 'sgd', 'kf-ngd']:\n",
    "        print(f\"\\n\\n=== Training with {opt_name} ===\")\n",
    "        model_trained, logs, final_rel = train_and_evaluate(\n",
    "            base_model, opt_name,\n",
    "            train_loader, test_loader,\n",
    "            num_epochs=num_epochs,\n",
    "            lr=1e-3, damping=0.01, gamma=0.05\n",
    "        )\n",
    "        results[opt_name] = {\n",
    "            'model': model_trained,\n",
    "            'logs': logs,\n",
    "            'final_rel': final_rel\n",
    "        }\n",
    "\n",
    "    print(\"\\n=== Training finished ===\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No results to plot. Run training first.\n"
     ]
    }
   ],
   "source": [
    "# Cell 8 graph\n",
    "\n",
    "def plot_results(results):\n",
    "    plt.figure()\n",
    "    for name, info in results.items():\n",
    "        logs = info['logs']\n",
    "        plt.plot(logs['train_loss'], label=name)\n",
    "    plt.xlabel(\"Epoch\")\n",
    "    plt.ylabel(\"Training MSE Loss\")\n",
    "    plt.yscale(\"log\")\n",
    "    plt.legend()\n",
    "    plt.title(\"Training Loss vs Epochs\")\n",
    "    plt.show()\n",
    "\n",
    "    for name, info in results.items():\n",
    "        print(f\"{name} final test Relative L2: {info['final_rel']:.6e}\")\n",
    "\n",
    "if 'results' in globals():\n",
    "    plot_results(results)\n",
    "else:\n",
    "    print(\"No results to plot. Run training first.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current memory usage: 77.76 MB\n",
      "Memory cleanup done!\n"
     ]
    }
   ],
   "source": [
    "# 清理数据\n",
    "import gc\n",
    "import sys\n",
    "\n",
    "for name in list(globals().keys()):\n",
    "    if not name.startswith(\"_\") and name not in [\"gc\", \"sys\"]:\n",
    "        del globals()[name]\n",
    "\n",
    "gc.collect()\n",
    "\n",
    "try:\n",
    "    import psutil, os\n",
    "    process = psutil.Process(os.getpid())\n",
    "    print(f\"Current memory usage: {process.memory_info().rss / 1024 ** 2:.2f} MB\")\n",
    "except ImportError:\n",
    "    print(\"psutil not installed; skipped memory usage check.\")\n",
    "\n",
    "print(\"Memory cleanup done!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
