{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(10, 20)\n",
    "        self.fc2 = nn.Linear(20, 1)\n",
    "    def forward(self, x):\n",
    "        h = torch.relu(self.fc1(x))\n",
    "        out = self.fc2(h)\n",
    "        return out, h"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class KFNGD:\n",
    "    def __init__(self, model, lr=1e-3, gamma=0.05, damping=1e-3):\n",
    "        self.model = model\n",
    "        self.lr = lr\n",
    "        self.gamma = gamma\n",
    "        self.damping = damping\n",
    "        self.A_hat = None\n",
    "        self.G_hat = None\n",
    "\n",
    "    def step(self, loss, activations):\n",
    "        loss.backward()\n",
    "        grads = []\n",
    "        for p in self.model.parameters():\n",
    "            if p.grad is not None:\n",
    "                grads.append(p.grad.view(-1))\n",
    "        g_bar = torch.cat(grads).detach()\n",
    "        a_bar = activations.detach()\n",
    "\n",
    "        # 计算 batch 协方差 (这里只写单样本版；batch 需按公式求和后 /B)\n",
    "        A_batch = torch.outer(a_bar, a_bar)\n",
    "        G_batch = torch.outer(g_bar, g_bar)\n",
    "\n",
    "        if self.A_hat is None:\n",
    "            self.A_hat, self.G_hat = A_batch, G_batch\n",
    "        else:\n",
    "            self.A_hat = (1-self.gamma)*self.A_hat + self.gamma*A_batch\n",
    "            self.G_hat = (1-self.gamma)*self.G_hat + self.gamma*G_batch\n",
    "            \n",
    "        I_A = torch.eye(self.A_hat.shape[0])\n",
    "        I_G = torch.eye(self.G_hat.shape[0])\n",
    "        A_inv = torch.linalg.inv(self.A_hat + self.damping * I_A)\n",
    "        G_inv = torch.linalg.inv(self.G_hat + self.damping * I_G)\n",
    "\n",
    "        # natural gradient\n",
    "        a_tilde = A_inv @ a_bar\n",
    "        g_tilde = G_inv @ g_bar\n",
    "        g_nat = torch.kron(a_tilde, g_tilde)\n",
    "\n",
    "        # 更新参数\n",
    "        offset = 0\n",
    "        for p in self.model.parameters():\n",
    "            if p.grad is not None:\n",
    "                n = p.numel()\n",
    "                p.data -= self.lr * g_nat[offset:offset+n].view_as(p)\n",
    "                offset += n\n",
    "\n",
    "        self.model.zero_grad()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step 0 loss=1.8181\n",
      "step 1 loss=0.8767\n",
      "step 2 loss=0.2927\n",
      "step 3 loss=0.8811\n",
      "step 4 loss=0.5779\n",
      "step 5 loss=0.6285\n",
      "step 6 loss=1.7943\n",
      "step 7 loss=0.9657\n",
      "step 8 loss=1.6610\n",
      "step 9 loss=1.2665\n",
      "step 10 loss=0.5698\n",
      "step 11 loss=0.9160\n",
      "step 12 loss=0.7589\n",
      "step 13 loss=0.3513\n",
      "step 14 loss=0.2464\n",
      "step 15 loss=0.3675\n",
      "step 16 loss=0.3665\n",
      "step 17 loss=1.0077\n",
      "step 18 loss=0.2663\n",
      "step 19 loss=0.3218\n",
      "step 20 loss=1.6320\n",
      "step 21 loss=0.3754\n",
      "step 22 loss=0.8376\n",
      "step 23 loss=0.7988\n",
      "step 24 loss=1.0424\n",
      "step 25 loss=1.7393\n",
      "step 26 loss=0.1959\n",
      "step 27 loss=0.6837\n",
      "step 28 loss=1.1714\n",
      "step 29 loss=2.7500\n",
      "step 30 loss=0.8828\n",
      "step 31 loss=1.2167\n",
      "step 32 loss=1.2394\n",
      "step 33 loss=2.4025\n",
      "step 34 loss=1.1102\n",
      "step 35 loss=0.4996\n",
      "step 36 loss=0.2793\n",
      "step 37 loss=0.9932\n",
      "step 38 loss=1.8271\n",
      "step 39 loss=1.2606\n",
      "step 40 loss=0.6624\n",
      "step 41 loss=1.0632\n",
      "step 42 loss=0.7606\n",
      "step 43 loss=0.8815\n",
      "step 44 loss=1.5093\n",
      "step 45 loss=3.5758\n",
      "step 46 loss=1.8410\n",
      "step 47 loss=1.2291\n",
      "step 48 loss=0.7034\n",
      "step 49 loss=0.1855\n",
      "step 50 loss=0.1007\n",
      "step 51 loss=1.3767\n",
      "step 52 loss=0.8453\n",
      "step 53 loss=1.3314\n",
      "step 54 loss=2.3907\n",
      "step 55 loss=0.1416\n",
      "step 56 loss=1.2572\n",
      "step 57 loss=1.9092\n",
      "step 58 loss=0.7296\n",
      "step 59 loss=0.1888\n",
      "step 60 loss=0.2342\n",
      "step 61 loss=1.3181\n",
      "step 62 loss=0.6518\n",
      "step 63 loss=1.0701\n",
      "step 64 loss=1.1272\n",
      "step 65 loss=1.9026\n",
      "step 66 loss=0.7473\n",
      "step 67 loss=0.3769\n",
      "step 68 loss=0.0387\n",
      "step 69 loss=2.6611\n",
      "step 70 loss=0.5900\n",
      "step 71 loss=2.1856\n",
      "step 72 loss=0.1085\n",
      "step 73 loss=2.5409\n",
      "step 74 loss=0.8442\n",
      "step 75 loss=2.8440\n",
      "step 76 loss=1.5232\n",
      "step 77 loss=4.3205\n",
      "step 78 loss=1.7762\n",
      "step 79 loss=0.6354\n",
      "step 80 loss=0.1548\n",
      "step 81 loss=0.6946\n",
      "step 82 loss=1.4711\n",
      "step 83 loss=1.0209\n",
      "step 84 loss=0.7748\n",
      "step 85 loss=1.8012\n",
      "step 86 loss=1.5914\n",
      "step 87 loss=0.2428\n",
      "step 88 loss=0.5376\n",
      "step 89 loss=1.0540\n",
      "step 90 loss=0.5606\n",
      "step 91 loss=0.3922\n",
      "step 92 loss=2.7258\n",
      "step 93 loss=1.1890\n",
      "step 94 loss=1.0447\n",
      "step 95 loss=0.6871\n",
      "step 96 loss=0.7976\n",
      "step 97 loss=1.3685\n",
      "step 98 loss=1.0167\n",
      "step 99 loss=1.3903\n",
      "step 100 loss=1.0819\n",
      "step 101 loss=0.4305\n",
      "step 102 loss=0.7366\n",
      "step 103 loss=0.2416\n",
      "step 104 loss=2.1179\n",
      "step 105 loss=0.0967\n",
      "step 106 loss=0.9108\n",
      "step 107 loss=0.3290\n",
      "step 108 loss=1.3069\n",
      "step 109 loss=0.0453\n",
      "step 110 loss=1.9143\n",
      "step 111 loss=1.8931\n",
      "step 112 loss=0.3458\n",
      "step 113 loss=0.5980\n",
      "step 114 loss=1.7338\n",
      "step 115 loss=1.3858\n",
      "step 116 loss=0.7251\n",
      "step 117 loss=1.5783\n",
      "step 118 loss=0.7989\n",
      "step 119 loss=0.8522\n",
      "step 120 loss=1.6653\n",
      "step 121 loss=1.2965\n",
      "step 122 loss=0.2976\n",
      "step 123 loss=0.4939\n",
      "step 124 loss=0.6109\n",
      "step 125 loss=1.3464\n",
      "step 126 loss=1.2212\n",
      "step 127 loss=1.1807\n",
      "step 128 loss=0.6918\n",
      "step 129 loss=3.1489\n",
      "step 130 loss=2.4429\n",
      "step 131 loss=0.6517\n",
      "step 132 loss=0.5187\n",
      "step 133 loss=1.9658\n",
      "step 134 loss=3.0477\n",
      "step 135 loss=0.6992\n",
      "step 136 loss=0.5071\n",
      "step 137 loss=0.3594\n",
      "step 138 loss=0.6702\n",
      "step 139 loss=1.1644\n",
      "step 140 loss=1.0489\n",
      "step 141 loss=0.9894\n",
      "step 142 loss=1.2732\n",
      "step 143 loss=1.7486\n",
      "step 144 loss=0.4053\n",
      "step 145 loss=0.9172\n",
      "step 146 loss=0.7951\n",
      "step 147 loss=0.0947\n",
      "step 148 loss=0.6455\n",
      "step 149 loss=2.7231\n",
      "step 150 loss=0.4759\n",
      "step 151 loss=1.0500\n",
      "step 152 loss=1.2456\n",
      "step 153 loss=1.9796\n",
      "step 154 loss=0.9757\n",
      "step 155 loss=2.3213\n",
      "step 156 loss=0.4080\n",
      "step 157 loss=0.7760\n",
      "step 158 loss=0.6708\n",
      "step 159 loss=3.0546\n",
      "step 160 loss=3.1720\n",
      "step 161 loss=0.6393\n",
      "step 162 loss=2.0119\n",
      "step 163 loss=1.3256\n",
      "step 164 loss=0.7771\n",
      "step 165 loss=0.7576\n",
      "step 166 loss=0.1787\n",
      "step 167 loss=0.0883\n",
      "step 168 loss=1.3042\n",
      "step 169 loss=4.6268\n",
      "step 170 loss=1.7904\n",
      "step 171 loss=0.6331\n",
      "step 172 loss=0.4086\n",
      "step 173 loss=3.9053\n",
      "step 174 loss=1.3942\n",
      "step 175 loss=0.3818\n",
      "step 176 loss=0.9661\n",
      "step 177 loss=0.6103\n",
      "step 178 loss=0.4783\n",
      "step 179 loss=0.9617\n",
      "step 180 loss=0.7804\n",
      "step 181 loss=2.1681\n",
      "step 182 loss=3.6154\n",
      "step 183 loss=0.2674\n",
      "step 184 loss=1.1264\n",
      "step 185 loss=0.4295\n",
      "step 186 loss=0.6842\n",
      "step 187 loss=0.4924\n",
      "step 188 loss=1.3864\n",
      "step 189 loss=0.3498\n",
      "step 190 loss=0.8569\n",
      "step 191 loss=1.0859\n",
      "step 192 loss=1.6570\n",
      "step 193 loss=1.4549\n",
      "step 194 loss=0.7671\n",
      "step 195 loss=0.5965\n",
      "step 196 loss=0.7695\n",
      "step 197 loss=1.6410\n",
      "step 198 loss=1.5857\n",
      "step 199 loss=2.5312\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(0)\n",
    "net = SimpleNet()\n",
    "opt = KFNGD(net, lr=1e-3)\n",
    "\n",
    "for step in range(200): \n",
    "    x = torch.randn(4, 10)\n",
    "    y = torch.randn(4, 1)\n",
    "    out, h = net(x)\n",
    "    loss = ((out - y) ** 2).mean()\n",
    "\n",
    "    opt.step(loss, h.mean(0))\n",
    "\n",
    "    print(f\"step {step} loss={loss.item():.4f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
